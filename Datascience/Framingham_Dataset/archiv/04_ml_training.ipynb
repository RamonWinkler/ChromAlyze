{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning algorithm testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "Ab_cyx0nxw8B",
    "outputId": "4989e995-c484-442c-8cc9-01e07dbe1b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_framingham.csv\n",
      "01_Data_preprocessing.ipynb\n",
      "01_framingham_clean.csv\n",
      "02_framingham_oversampling_scaled_train.csv\n",
      "02_framingham_oversampling_train.csv\n",
      "02_framingham_scaled_smote_train.csv\n",
      "02_framingham_scaled_test.csv\n",
      "02_framingham_scaled_train.csv\n",
      "02_framingham_scaled_undersampling_train.csv\n",
      "02_framingham_smote_train.csv\n",
      "02_framingham_test.csv\n",
      "02_framingham_undersampling_train.csv\n",
      "02_Statistical_analyse.ipynb\n",
      "04_ml_training.ipynb\n",
      "05_risk_factors.ipynb\n",
      "cvd_risk_model.pkl\n",
      "feature_importances.csv\n",
      "ml_training_smote.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "for file in os.listdir():\n",
    "    print(file)\n",
    "\n",
    "df_train= pd.read_csv(\"02_framingham_undersampling_train.csv\")\n",
    "df_test= pd.read_csv(\"02_framingham_scaled_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TenYearCHD\n",
      "0    2871\n",
      "1    2871\n",
      "Name: count, dtype: int64\n",
      "Size of no CVD Risk 50.0 %\n",
      "Size of no CVD Risk 50.0 %\n"
     ]
    }
   ],
   "source": [
    "sizes = df_train['TenYearCHD'].value_counts()\n",
    "print(sizes)\n",
    "print('Size of no CVD Risk',np.round(100/sizes.sum()*sizes[0],2),'%')\n",
    "print('Size of no CVD Risk',np.round(100/sizes.sum()*sizes[1],2),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8odTh4gyR1X"
   },
   "outputs": [],
   "source": [
    "# Split data in Train and Test and x and y\n",
    "X_train = df_train.drop([\"TenYearCHD\"], axis = 1)\n",
    "y_train = df_train[\"TenYearCHD\"]\n",
    "\n",
    "X_test = df_test.drop([\"TenYearCHD\"], axis = 1)\n",
    "y_test = df_test[\"TenYearCHD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "HI40sYCxxw8F",
    "outputId": "4d8cbcc1-63d0-4164-cb3f-d6b95e9afb51"
   },
   "outputs": [],
   "source": [
    "# combine x_train and y_train\n",
    "train_set = pd.merge(X_train, y_train, left_index=True, right_index=True)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "UrDsImMoxw8G",
    "outputId": "9c1e4f27-9248-4dce-aa92-e8bd6c88a3e7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,6))\n",
    "mask = np.triu(np.ones_like(train_set.corr().abs(), dtype= bool))\n",
    "\n",
    "heatmap = sns.heatmap(train_set.corr().abs(), mask= mask, vmin= 0, vmax= 0.6, annot= True, cmap= \"YlGnBu\", fmt= \".2f\")\n",
    "heatmap.set_title(\"Triangle Correlation Heatmap\", fontdict= {\"fontsize\": 18}, pad= 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "7bbkaJo1xw8G",
    "outputId": "c05b2237-4efe-455c-aa84-2192a39a2a5e"
   },
   "outputs": [],
   "source": [
    "# correlation of indepenedent variables with the dependent variable\n",
    "plt.figure(figsize=(6,8))\n",
    "correlation = train_set.corr()[[\"TenYearCHD\"]].abs().sort_values(by= \"TenYearCHD\", ascending= False)\n",
    "correlation = correlation[correlation < 1]\n",
    "heatmap = sns.heatmap(correlation, annot= True, cmap= \"YlGnBu\")\n",
    "heatmap.set_title(\"Correlation of Independent Variables with the Dependent Variable\", fontdict= {\"fontsize\": 18}, pad= 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJP_5iyIxw8H"
   },
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fhCRv5Mxw8H"
   },
   "outputs": [],
   "source": [
    "# import all necessary models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crk1RWuaxw8H",
    "outputId": "de24eb4f-5aed-4bd5-ebd4-6ad9729c4e44"
   },
   "outputs": [],
   "source": [
    "# model 1\n",
    "m1 = \"Logistic Regression\"\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_predict = lr.predict(X_test)\n",
    "lr_conf_matrix = confusion_matrix(y_test, lr_predict)\n",
    "lr_acc_score = accuracy_score(y_test, lr_predict)\n",
    "print(\"confussion matrix\")\n",
    "print(lr_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Logistic Regression model:\",lr_acc_score*100, \"\\n\")\n",
    "print(classification_report(y_test, lr_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ajkYTy1ixw8H",
    "outputId": "5f7879da-8208-402a-dd3d-e94b3cf1c74f"
   },
   "outputs": [],
   "source": [
    "# model 2\n",
    "m2 = \"Naive Bayes\"\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "nb_conf_matrix = confusion_matrix(y_test, nb_pred)\n",
    "nb_acc_score = accuracy_score(y_test, nb_pred)\n",
    "print(\"confussion matrix\")\n",
    "print(nb_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Naive Bayes model:\", nb_acc_score*100, \"\\n\")\n",
    "print(classification_report(y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1IXZffmxw8I",
    "outputId": "c8042057-6b71-419b-8d36-b1b6d6fde772"
   },
   "outputs": [],
   "source": [
    "# model 3\n",
    "m3 = \"Random Forest Classifier\"\n",
    "rf = RandomForestClassifier(n_estimators=20, random_state=12, max_depth=5)\n",
    "rf.fit(X_train,y_train)\n",
    "rf_predicted = rf.predict(X_test)\n",
    "rf_conf_matrix = confusion_matrix(y_test, rf_predicted)\n",
    "rf_acc_score = accuracy_score(y_test, rf_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(rf_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Random Forest model:\", rf_acc_score*100, \"\\n\")\n",
    "print(classification_report(y_test, rf_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "iZGEIeHrxw8I",
    "outputId": "443a8c97-aa3c-436d-c314-bd717aa9ac7e"
   },
   "outputs": [],
   "source": [
    "# Calculate and plot feature importance\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "feature_importances.sort_values().plot(kind='barh')  # Using sort_values() for correct sorting\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.show()\n",
    "\n",
    "# Save feature importances to CSV\n",
    "feature_importances.to_csv(\"feature_importances.csv\")\n",
    "\n",
    "# Save the trained CVD risk model as a .pkl file\n",
    "import joblib\n",
    "cvd_risk_model = rf  # rf is your trained random forest model\n",
    "joblib.dump(cvd_risk_model, \"cvd_risk_model.pkl\")\n",
    "\n",
    "print(\"Feature importances and model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Ao4utghxw8I",
    "outputId": "b5448e21-8c9c-4a70-f91d-0660f91d65e5"
   },
   "outputs": [],
   "source": [
    "m4 = 'Extreme Gradient Boost'\n",
    "xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27,\n",
    "                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_predicted = xgb.predict(X_test)\n",
    "xgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\n",
    "xgb_acc_score = accuracy_score(y_test, xgb_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(xgb_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\n",
    "print(classification_report(y_test,xgb_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GicbgAzxw8I",
    "outputId": "64e69563-4087-4c7a-ce57-d967b8b4b386"
   },
   "outputs": [],
   "source": [
    "m5 = 'K-NeighborsClassifier'\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_predicted = knn.predict(X_test)\n",
    "knn_conf_matrix = confusion_matrix(y_test, knn_predicted)\n",
    "knn_acc_score = accuracy_score(y_test, knn_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(knn_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\n",
    "print(classification_report(y_test,knn_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1O6pnnZRxw8I",
    "outputId": "66292b95-9c55-4f93-ccdd-5c59ef35597e"
   },
   "outputs": [],
   "source": [
    "m6 = 'DecisionTreeClassifier'\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_predicted = dt.predict(X_test)\n",
    "dt_conf_matrix = confusion_matrix(y_test, dt_predicted)\n",
    "dt_acc_score = accuracy_score(y_test, dt_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(dt_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\n",
    "print(classification_report(y_test,dt_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkBntQUbxw8I",
    "outputId": "339a2c95-256d-4152-a48c-493dec4f1f90"
   },
   "outputs": [],
   "source": [
    "m7 = 'Support Vector Classifier'\n",
    "svc =  SVC(kernel='rbf', C=2)\n",
    "svc.fit(X_train, y_train)\n",
    "svc_predicted = svc.predict(X_test)\n",
    "svc_conf_matrix = confusion_matrix(y_test, svc_predicted)\n",
    "svc_acc_score = accuracy_score(y_test, svc_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(svc_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\n",
    "print(classification_report(y_test,svc_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfwQvHkRxw8I",
    "outputId": "7c7bcc07-2a97-4d71-9c86-1fd0aec6a8fc"
   },
   "outputs": [],
   "source": [
    "# fine tuning the random forest model (Manual from towards data science)\n",
    "# model 3\n",
    "m3 = \"Random Forest Classifier\"\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "rf_predicted = rf.predict(X_test)\n",
    "rf_conf_matrix = confusion_matrix(y_test, rf_predicted)\n",
    "rf_acc_score = accuracy_score(y_test, rf_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(rf_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Random Forest model:\", rf_acc_score*100, \"\\n\")\n",
    "print(classification_report(y_test, rf_predicted))\n",
    "\n",
    "# with pprint you can see the default parameters of a model:\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cAoaI-KVxw8I",
    "outputId": "fa579334-1caa-4949-ad72-1919f364cde3"
   },
   "outputs": [],
   "source": [
    "# tuning a model, using RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [100, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# random grid for the predefined parameters\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HvUqEQJpxw8I",
    "outputId": "73e863f5-a8d7-431e-effa-39642efdb9c4"
   },
   "outputs": [],
   "source": [
    "# with this one can use the random grid to search for best hyperparameters\n",
    "# first create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# fit the random search\n",
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee0NiC8Uxw8I"
   },
   "source": [
    "# documentation of random search CV\n",
    "n_iter: controls the number of different combinations to try\n",
    "\n",
    "cv: which number of folds to use for cross validation\n",
    "\n",
    "More iterations = wider search space, more cv folds reduces the chance of overfitting, increasing leads to increaed run times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqE_9ec3xw8I",
    "outputId": "3433ee86-28d0-44ca-aa4d-1499c9ecd2c9"
   },
   "outputs": [],
   "source": [
    "# view the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZCyGZW1xw8J",
    "outputId": "de86d140-b86f-4041-a2b4-fcb9a0a23628"
   },
   "outputs": [],
   "source": [
    "# evaluate the optimized model\n",
    "m3 = \"Random Forest Classifier\"\n",
    "rf2 = RandomForestClassifier(n_estimators = 1600, min_samples_split = 2, min_samples_leaf = 4, max_features = \"sqrt\", max_depth = 20, bootstrap = True, random_state=42)\n",
    "rf2.fit(X_train,y_train)\n",
    "rf2_predicted = rf2.predict(X_test)\n",
    "rf2_conf_matrix = confusion_matrix(y_test, rf2_predicted)\n",
    "rf2_acc_score = accuracy_score(y_test, rf2_predicted)\n",
    "print(\"confussion matrix\")\n",
    "print(rf2_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Random Forest model:\", rf2_acc_score*100, \"\\n\")\n",
    "print(classification_report(y_test, rf2_predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "VjLJZuSExw8J",
    "outputId": "267740c6-ce15-472e-a8e4-6d6352628443"
   },
   "outputs": [],
   "source": [
    "# feature importance --> to be optimized this based on random forest\n",
    "importances = rf2.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Feature importances using Random Forest\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]) ,features)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "KG9ygAIcxw8J",
    "outputId": "711e39ce-38ba-4338-ad80-a4722f98990f"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define refined hyperparameters\n",
    "n_estimators = [100, 300, 500, 700, 1000]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "max_depth = [10, 20, 30, 40, 50]\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Refined random grid\n",
    "random_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'bootstrap': bootstrap\n",
    "}\n",
    "\n",
    "# Instantiate RandomForest and RandomizedSearchCV\n",
    "rf3 = RandomForestClassifier(random_state=42)\n",
    "rf3_random = RandomizedSearchCV(estimator=rf3, param_distributions=random_grid,\n",
    "                               n_iter=50, cv=5, verbose=2, random_state=42,\n",
    "                               n_jobs=-1, scoring='roc_auc')\n",
    "\n",
    "# Fit the model\n",
    "rf3_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb7KhybTxw8J"
   },
   "outputs": [],
   "source": [
    "rf3_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N0oYMuxxxw8J",
    "outputId": "3680edc8-bdd2-48f1-9787-63f0d73c1baa"
   },
   "outputs": [],
   "source": [
    "# evaluate the optimized model again\n",
    "\n",
    "rf4 = RandomForestClassifier(n_estimators = 1000, min_samples_split = 2, min_samples_leaf = 1, max_features = \"sqrt\", max_depth = 10, bootstrap = True, random_state=42)\n",
    "rf4.fit(X_train,y_train)\n",
    "rf4_predicted = rf4.predict(X_test)\n",
    "rf4_conf_matrix = confusion_matrix(y_test, rf4_predicted)\n",
    "rf4_acc_score = accuracy_score(y_test, rf4_predicted)\n",
    "\n",
    "print(\"confussion matrix\")\n",
    "print(rf2_conf_matrix)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of Random Forest model:\", rf4_acc_score*100, \"\\n\")\n",
    "print(classification_report(y_test, rf4_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "_g0GIcqTxw8J",
    "outputId": "735bd7f1-55ed-497e-d3b0-a7d0d8c900f0"
   },
   "outputs": [],
   "source": [
    "# feature importance --> to be optimized this based on random forest\n",
    "importances = rf4.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Feature importances using Random Forest\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "plt.xticks(range(X_train.shape[1]) ,features)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2IPmeRsxw8J"
   },
   "source": [
    "# safe the model and feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJBC_CLVxw8J"
   },
   "outputs": [],
   "source": [
    "cvd_risk_model = rf\n",
    "# save the model as pkl\n",
    "\n",
    "import joblib\n",
    "joblib.dump(cvd_risk_model, \"cvd_risk_model.pkl\")\n",
    "\n",
    "\n",
    "# save the nodel importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUi8vkCyxw8J"
   },
   "outputs": [],
   "source": [
    "# emr bots! artificial generated electronics medical records\n",
    "# cdss in r or ython,\n",
    "\n",
    "# exercise folder moodle\n",
    "\n",
    "# 100 patients folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emmZAbphzz46"
   },
   "outputs": [],
   "source": [
    "# min max scaler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations = 4, population_size=10, verbosity = 3 )\n",
    "fitting = tpot.fit(X_train, y_train)\n",
    "# print('TPOT Score: ',tpot.score(X_test, y_test))\n",
    "y_pred = tpot.predict(X_test)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
